{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "209e5f68-460e-4ff9-9fa8-0d925f3bba24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras import layers, models, regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b084db4-2035-465b-a68a-1562d3030a20",
      "metadata": {},
      "source": [
        "## Setting the DataFrame and extracting the audio path and JSON metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "be3a07ba-5052-41e4-b895-c98bee733947",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_path</th>\n",
              "      <th>cough_confidence</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>../public_dataset\\2d1d8b3f-4bfc-4bbd-b32e-1a03...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>../public_dataset\\2d22665d-5ca0-47f1-80ad-ebfa...</td>\n",
              "      <td>0.8796</td>\n",
              "      <td>2020-08-19T18:12:19.693957+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>../public_dataset\\2d25616a-2bee-492e-a3ff-7c20...</td>\n",
              "      <td>0.0099</td>\n",
              "      <td>2020-04-14T18:37:54.848372+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>../public_dataset\\2d26833b-337e-4d31-b566-f0d2...</td>\n",
              "      <td>0.9817</td>\n",
              "      <td>2020-05-18T19:56:19.359410+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>../public_dataset\\2d28744c-a394-438b-8999-0542...</td>\n",
              "      <td>0.8822</td>\n",
              "      <td>2020-04-14T15:04:24.762414+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           file_path  cough_confidence  \\\n",
              "0  ../public_dataset\\2d1d8b3f-4bfc-4bbd-b32e-1a03...            0.0000   \n",
              "1  ../public_dataset\\2d22665d-5ca0-47f1-80ad-ebfa...            0.8796   \n",
              "2  ../public_dataset\\2d25616a-2bee-492e-a3ff-7c20...            0.0099   \n",
              "3  ../public_dataset\\2d26833b-337e-4d31-b566-f0d2...            0.9817   \n",
              "4  ../public_dataset\\2d28744c-a394-438b-8999-0542...            0.8822   \n",
              "\n",
              "                          timestamp  \n",
              "0                           unknown  \n",
              "1  2020-08-19T18:12:19.693957+00:00  \n",
              "2  2020-04-14T18:37:54.848372+00:00  \n",
              "3  2020-05-18T19:56:19.359410+00:00  \n",
              "4  2020-04-14T15:04:24.762414+00:00  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# setting the dataset folder\n",
        "folder_path = r\"../public_dataset\"\n",
        "\n",
        "# Creating lists to store the files\n",
        "\n",
        "files = []\n",
        "cough_scores = []\n",
        "timestamps = []\n",
        "\n",
        "# loop through all files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".wav\"):\n",
        "        # store the wav file path\n",
        "        wav_path = os.path.join(folder_path, file_name)\n",
        "        files.append(wav_path)\n",
        "\n",
        "        # Get the corresponding JSON file \n",
        "        json_name = file_name.replace(\".wav\", \".json\")\n",
        "        json_path = os.path.join(folder_path, json_name)\n",
        "\n",
        "        # Read the labels from the JSON\n",
        "        if os.path.exists(json_path):\n",
        "            with open(json_path, \"r\") as f:\n",
        "                # store the json data in the data dictionary\n",
        "                data = json.load(f)\n",
        "\n",
        "                cough_scores.append(float(data[\"cough_detected\"]))\n",
        "                timestamps.append(data[\"datetime\"])\n",
        "        else:\n",
        "            cough_scores.append(0.0)\n",
        "            timestamps.append(\"unknown\")\n",
        "            \n",
        "# Creating the dataframe\n",
        "df = pd.DataFrame({\n",
        "    \"file_path\" : files,\n",
        "    \"cough_confidence\" : cough_scores,\n",
        "    \"timestamp\" : timestamps\n",
        "})\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dbe707f2-8d50-4a1a-a783-4327b30904d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_mfcc_2d(wav_path,\n",
        "                    sr = 16000,\n",
        "                    n_mfcc = 40,\n",
        "                    n_fft = 1024, \n",
        "                    hop_length = 512, \n",
        "                    duration = None, \n",
        "                    max_frames = None,\n",
        "                    normalise = True\n",
        "                   ):\n",
        "    \"\"\"\n",
        "    We Extract 2D MFCC (time x n_mfcc) from a wav file.\n",
        "\n",
        "    Parameters:\n",
        "    - wav_path: path to the .wav file\n",
        "    - sr: target sample rate\n",
        "    - n_mfcc: number of mfccs coefficient to be extracted\n",
        "    - n_fft: FFT window size (samples per each frame)\n",
        "    - hop_length: hop length(samples) between frames\n",
        "    \"\"\"\n",
        "    if duration is not None:\n",
        "        target_samples = int(sr * duration)\n",
        "        y, _ = librosa.load(wav_path, sr = sr, mono = True, duration = duration)\n",
        "        if len(y) < target_samples:\n",
        "             y = np.pad(y, (0, target_samples - len(y)), mode='constant')\n",
        "        else:\n",
        "            y, _ = librosa.load(wav_path, sr = sr, mono = True)\n",
        "\n",
        "    # (n_features, n_frames)\n",
        "    mfcc = librosa.feature.mfcc(y = y, sr = sr, n_mfcc = n_mfcc, n_fft = n_fft,\n",
        "                                hop_length = hop_length)\n",
        "\n",
        "    if normalise:\n",
        "        eps = 1e-9\n",
        "        mean = np.mean(mfcc, axis = 1, keepdims = True)\n",
        "        std = np.std(mfcc, axis = 1, keepdims = True)\n",
        "        mfcc = (mfcc - mean) / (std + eps)\n",
        "\n",
        "    # Transpose to (time_frames, n_features)                            \n",
        "    mfcc = mfcc.T  # shape: (n_frames, n_features)\n",
        "    \n",
        "    if max_frames is not None:\n",
        "        T, F = mfcc.shape\n",
        "        if T < max_frames:\n",
        "            pad_width = ((0, max_frames - T), (0, 0))\n",
        "            mfcc = np.pad(mfcc, pad_width=pad_width, mode='constant', constant_values=0.0)\n",
        "        elif T > max_frames:\n",
        "            mfcc = mfcc[:max_frames, :] \n",
        "\n",
        "    return mfcc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "da0a4c00-c83f-4cab-8b4a-566e000471f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting MFCCs: 100%|██████████| 22771/22771 [12:48<00:00, 29.65it/s]\n"
          ]
        }
      ],
      "source": [
        "def extract_features(df,\n",
        "                     duration = 9,\n",
        "                     sr = 16000,\n",
        "                     n_mfcc = 40,\n",
        "                     n_fft = 1024, \n",
        "                     hop_length = 512,\n",
        "                     normalise = True,\n",
        "                    ):\n",
        "    target_samples = int( sr * duration)\n",
        "    expected_frames = 1 + int(np.floor((target_samples - n_fft) / float(hop_length))) if target_samples > n_fft else 1\n",
        "    \n",
        "    n_files = len(df)\n",
        "    X = np.zeros((n_files, expected_frames, n_mfcc), dtype = np.float32)\n",
        "    y = (df[\"cough_confidence\"] >= 0.5).astype(np.float32).values\n",
        "\n",
        "    for i, wav_path in enumerate(tqdm(df[\"file_path\"], desc = \"Extracting MFCCs\")):\n",
        "        try:\n",
        "            mfcc = extract_mfcc_2d(\n",
        "                wav_path,\n",
        "                sr = sr,\n",
        "                n_mfcc = n_mfcc,\n",
        "                n_fft = n_fft,\n",
        "                hop_length = hop_length,\n",
        "                duration = duration,\n",
        "                max_frames=expected_frames,\n",
        "                normalise = normalise\n",
        "            )\n",
        "            X[i] = mfcc\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: failed to process {wav_path}: {e}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X, y = extract_features(df, duration = 9.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d813dac3-db5f-411b-9fec-750c282c0fea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (22771, 280, 40)\n",
            "y shape: (22771,)\n",
            "\n",
            "Class distribution:\n",
            "Cough samples (1): 15041\n",
            "Non-cough samples (0): 7730\n"
          ]
        }
      ],
      "source": [
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "print(f\"\\nClass distribution:\")\n",
        "\n",
        "print(f\"Cough samples (1): {np.sum(y == 1)}\")\n",
        "print(f\"Non-cough samples (0): {np.sum(y == 0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6dd57b92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version: 2.10.0\n",
            "GPU Available: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('Version:', tf.__version__); print('GPU Available:', len(tf.config.list_physical_devices('GPU')) > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "78c6871b-d479-46ea-b4f2-e0225683dbb3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: (14572, 280, 40)\n",
            "Test set: (4555, 280, 40)\n",
            "validating set: (3644, 280, 40)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size = 0.2,\n",
        "    random_state = 42,\n",
        "    stratify = y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"validating set: {X_val.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "44b84ee1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def time_shift(mfcc, max_shift=10):\n",
        "    \"\"\"Randomly roll MFCCs along time axis (frame shift).\"\"\"\n",
        "    shift = tf.random.uniform([], -max_shift, max_shift + 1, dtype=tf.int32)\n",
        "    return tf.roll(mfcc, shift=shift, axis=0)\n",
        "\n",
        "\n",
        "def add_noise(mfcc, noise_level=0.02):\n",
        "    \"\"\"Add small Gaussian noise without changing shape or scaling.\"\"\"\n",
        "    noise = tf.random.normal(tf.shape(mfcc), stddev=noise_level)\n",
        "    return mfcc + noise\n",
        "\n",
        "\n",
        "def random_gain(mfcc, min_gain=0.8, max_gain=1.2):\n",
        "    \"\"\"Apply a random global gain (volume-like scaling).\"\"\"\n",
        "    gain = tf.random.uniform([], min_gain, max_gain)\n",
        "    return mfcc * gain\n",
        "\n",
        "\n",
        "def augment_mfcc(mfcc, label):\n",
        "    \"\"\"Apply lightweight augmentations for training only.\n",
        "\n",
        "    Keeps the (time, n_mfcc) shape identical so the model\n",
        "    and Arduino preprocessing pipeline remain unchanged.\n",
        "    \"\"\"\n",
        "    # Random small time shift\n",
        "    if tf.random.uniform([]) < 0.5:\n",
        "        mfcc = time_shift(mfcc, max_shift=10)\n",
        "\n",
        "    # Random gain\n",
        "    if tf.random.uniform([]) < 0.5:\n",
        "        mfcc = random_gain(mfcc, min_gain=0.9, max_gain=1.1)\n",
        "\n",
        "    # Always add a bit of noise\n",
        "    mfcc = add_noise(mfcc, noise_level=0.01)\n",
        "\n",
        "    return mfcc, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6e87e321",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def augment_mfcc(mfcc, label):\n",
        "#     mfcc = time_shift(mfcc)\n",
        "#     mfcc = add_noise(mfcc)\n",
        "#     mfcc = random_gain(mfcc)\n",
        "\n",
        "#     if tf.random.uniform([]) < 0.5:\n",
        "#         mfcc = spec_augment(mfcc)\n",
        "\n",
        "#     return mfcc, label\n",
        "\n",
        "# train_ds = train_ds.map(augment_mfcc, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3a4623b8-e43c-4a98-8506-9082167bf020",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: (14572, 2)\n",
            "Validation set: (3644, 2)\n",
            "Test set: (4555, 2)\n"
          ]
        }
      ],
      "source": [
        "y_train_onehot = to_categorical(y_train, num_classes=2)\n",
        "y_val_onehot   = to_categorical(y_val,   num_classes=2)\n",
        "y_test_onehot  = to_categorical(y_test,  num_classes=2)\n",
        "\n",
        "print(f\"Training set: {y_train_onehot.shape}\")\n",
        "print(f\"Validation set: {y_val_onehot.shape}\")\n",
        "print(f\"Test set: {y_test_onehot.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fcdde9ef-0913-489a-ad04-158c4c438a86",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tinyml_cnn(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv1D(32, 3, padding='same', activation='relu')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"cough_cnn\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "48abb529-e3e2-4a4f-ae2e-187d6a66f295",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compile_model(model, learning_rate = 3e-4):\n",
        "    model.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "        loss = keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "        metrics=[\"accuracy\",\n",
        "                 keras.metrics.Precision(name=\"Precision\"),\n",
        "                 keras.metrics.Recall(name=\"recall\")]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "73d0cb27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. Final training data shape for model.fit: (14572, 280, 40)\n"
          ]
        }
      ],
      "source": [
        "print(f\"2. Final training data shape for model.fit: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0291e610-4425-4a34-95e1-831aa6cee40c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model,\n",
        "                X_train, y_train_onehot,\n",
        "                X_val, y_val_onehot,\n",
        "                class_weight=None,\n",
        "                batch_size=16,\n",
        "                epochs=50,\n",
        "                save_path=None):\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_onehot))\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=1024)\n",
        "\n",
        "        # Apply augmentation ONLY on the training dataset.\n",
        "        # This operates on already-extracted MFCCs, so the\n",
        "        # input shape (280, 40) and preprocessing pipeline\n",
        "        # used on Arduino stay exactly the same.\n",
        "        train_dataset = train_dataset.map(\n",
        "            augment_mfcc,\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        train_dataset = train_dataset.batch(batch_size)\n",
        "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        # Validation set is kept clean (no augmentation)\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val_onehot))\n",
        "        val_dataset = val_dataset.batch(batch_size)\n",
        "        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=8,\n",
        "            restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\",\n",
        "            factor=0.5,\n",
        "            patience=4,\n",
        "            min_lr=1e-6)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    if save_path:\n",
        "        model.save(save_path)\n",
        "        print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "78e626be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. Final training data shape for model.fit: (14572, 280, 40)\n"
          ]
        }
      ],
      "source": [
        "print(f\"2. Final training data shape for model.fit: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2ea7d6bf-71c3-46ab-aa0c-81c4e1bb17a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "456/456 [==============================] - 31s 34ms/step - loss: 0.5088 - accuracy: 0.7904 - Precision: 0.7904 - recall: 0.7904 - val_loss: 0.4153 - val_accuracy: 0.8620 - val_Precision: 0.8620 - val_recall: 0.8620 - lr: 3.0000e-04\n",
            "Epoch 2/50\n",
            "456/456 [==============================] - 17s 37ms/step - loss: 0.4297 - accuracy: 0.8590 - Precision: 0.8590 - recall: 0.8590 - val_loss: 0.3868 - val_accuracy: 0.8784 - val_Precision: 0.8784 - val_recall: 0.8784 - lr: 3.0000e-04\n",
            "Epoch 3/50\n",
            "456/456 [==============================] - 15s 33ms/step - loss: 0.4033 - accuracy: 0.8778 - Precision: 0.8778 - recall: 0.8778 - val_loss: 0.3876 - val_accuracy: 0.8762 - val_Precision: 0.8762 - val_recall: 0.8762 - lr: 3.0000e-04\n",
            "Epoch 4/50\n",
            "456/456 [==============================] - 15s 33ms/step - loss: 0.3893 - accuracy: 0.8883 - Precision: 0.8883 - recall: 0.8883 - val_loss: 0.3725 - val_accuracy: 0.8858 - val_Precision: 0.8858 - val_recall: 0.8858 - lr: 3.0000e-04\n",
            "Epoch 5/50\n",
            "456/456 [==============================] - 16s 34ms/step - loss: 0.3810 - accuracy: 0.8915 - Precision: 0.8915 - recall: 0.8915 - val_loss: 0.3708 - val_accuracy: 0.8891 - val_Precision: 0.8891 - val_recall: 0.8891 - lr: 3.0000e-04\n",
            "Epoch 6/50\n",
            "456/456 [==============================] - 17s 36ms/step - loss: 0.3732 - accuracy: 0.8956 - Precision: 0.8956 - recall: 0.8956 - val_loss: 0.3690 - val_accuracy: 0.8900 - val_Precision: 0.8900 - val_recall: 0.8900 - lr: 3.0000e-04\n",
            "Epoch 7/50\n",
            "456/456 [==============================] - 15s 34ms/step - loss: 0.3634 - accuracy: 0.9004 - Precision: 0.9004 - recall: 0.9004 - val_loss: 0.3789 - val_accuracy: 0.8850 - val_Precision: 0.8850 - val_recall: 0.8850 - lr: 3.0000e-04\n",
            "Epoch 8/50\n",
            "456/456 [==============================] - 16s 35ms/step - loss: 0.3603 - accuracy: 0.9047 - Precision: 0.9047 - recall: 0.9047 - val_loss: 0.3707 - val_accuracy: 0.8883 - val_Precision: 0.8883 - val_recall: 0.8883 - lr: 3.0000e-04\n",
            "Epoch 9/50\n",
            "456/456 [==============================] - 16s 34ms/step - loss: 0.3525 - accuracy: 0.9137 - Precision: 0.9137 - recall: 0.9137 - val_loss: 0.3710 - val_accuracy: 0.8894 - val_Precision: 0.8894 - val_recall: 0.8894 - lr: 3.0000e-04\n",
            "Epoch 10/50\n",
            "456/456 [==============================] - 15s 33ms/step - loss: 0.3500 - accuracy: 0.9136 - Precision: 0.9136 - recall: 0.9136 - val_loss: 0.3694 - val_accuracy: 0.8875 - val_Precision: 0.8875 - val_recall: 0.8875 - lr: 3.0000e-04\n",
            "Epoch 11/50\n",
            "456/456 [==============================] - 14s 31ms/step - loss: 0.3387 - accuracy: 0.9210 - Precision: 0.9210 - recall: 0.9210 - val_loss: 0.3716 - val_accuracy: 0.8902 - val_Precision: 0.8902 - val_recall: 0.8902 - lr: 1.5000e-04\n",
            "Epoch 12/50\n",
            "456/456 [==============================] - 9s 19ms/step - loss: 0.3329 - accuracy: 0.9256 - Precision: 0.9256 - recall: 0.9256 - val_loss: 0.3761 - val_accuracy: 0.8839 - val_Precision: 0.8839 - val_recall: 0.8839 - lr: 1.5000e-04\n",
            "Epoch 13/50\n",
            "456/456 [==============================] - 6s 14ms/step - loss: 0.3314 - accuracy: 0.9260 - Precision: 0.9260 - recall: 0.9260 - val_loss: 0.3714 - val_accuracy: 0.8875 - val_Precision: 0.8875 - val_recall: 0.8875 - lr: 1.5000e-04\n",
            "Epoch 14/50\n",
            "456/456 [==============================] - 8s 17ms/step - loss: 0.3259 - accuracy: 0.9291 - Precision: 0.9291 - recall: 0.9291 - val_loss: 0.3755 - val_accuracy: 0.8864 - val_Precision: 0.8864 - val_recall: 0.8864 - lr: 1.5000e-04\n",
            "Model saved to: cough_cnn1.h5\n"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "model = build_tinyml_cnn(input_shape=input_shape)\n",
        "compile_model(model, learning_rate=3e-4)\n",
        "history = train_model(model,\n",
        "                      X_train, y_train_onehot,\n",
        "                      X_val, y_val_onehot,\n",
        "                      batch_size=32,\n",
        "                      epochs=50,\n",
        "                      save_path=\"cough_cnn1.h5\")     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "aa304b0b-f879-4c1d-af5d-262231e4b6fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test_onehot, y_test_labels, batch_size=32, show_report=True):\n",
        "    \"\"\"Forces evaluation onto CPU to avoid GPU memory issues.\"\"\"\n",
        "    \n",
        "    # 1. Force TensorFlow operations to CPU\n",
        "    with tf.device('/cpu:0'):\n",
        "        # Create dataset on CPU\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_onehot))\n",
        "        test_dataset = test_dataset.batch(batch_size)\n",
        "        \n",
        "        # Move model weights to CPU temporarily\n",
        "        with tf.device('/cpu:0'):\n",
        "            # Evaluate\n",
        "            eval_metrics = model.evaluate(test_dataset, verbose=0)\n",
        "            print(\"Evaluation (loss, accuracy, precision, recall):\", eval_metrics)\n",
        "            \n",
        "            # Predict\n",
        "            y_pred_prob = model.predict(test_dataset, verbose=0)\n",
        "    \n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "    \n",
        "    if show_report:\n",
        "        print(\"\\nClassification report:\")\n",
        "        print(classification_report(y_test_labels, y_pred, digits=4))\n",
        "        print(\"Confusion matrix:\")\n",
        "        print(confusion_matrix(y_test_labels, y_pred))\n",
        "\n",
        "    return eval_metrics, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "46d9a2d2-b320-42e8-8973-7af3baaa23ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation (loss, accuracy, precision, recall): [0.3724524676799774, 0.887596070766449, 0.887596070766449, 0.887596070766449]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8301    0.8409    0.8355      1546\n",
            "         1.0     0.9177    0.9116    0.9146      3009\n",
            "\n",
            "    accuracy                         0.8876      4555\n",
            "   macro avg     0.8739    0.8762    0.8751      4555\n",
            "weighted avg     0.8880    0.8876    0.8878      4555\n",
            "\n",
            "Confusion matrix:\n",
            "[[1300  246]\n",
            " [ 266 2743]]\n"
          ]
        }
      ],
      "source": [
        "eval_metrics, y_pred = evaluate_model(model, X_test, y_test_onehot, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "525a6c5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 0.55 MB\n"
          ]
        }
      ],
      "source": [
        "def get_file_size(file_path):\n",
        "    \"\"\"Get file size in bytes\"\"\"\n",
        "    return os.path.getsize(file_path)\n",
        "\n",
        "def convert_bytes(bytes_size, unit):\n",
        "    \"\"\"Convert bytes to specified unit (KB, MB, GB)\"\"\"\n",
        "    units = {\"KB\": 1024, \"MB\": 1024**2, \"GB\": 1024**3}\n",
        "    return bytes_size / units[unit]\n",
        "\n",
        "# Now use it\n",
        "file_size_mb = convert_bytes(get_file_size(\"cough_cnn1.h5\"), \"MB\")\n",
        "print(f\"Model size: {file_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b9499f5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train saved as X_train.npy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Suppose your X_train is already loaded in memory\n",
        "np.save(\"X_train.npy\", X_train)\n",
        "print(\"X_train saved as X_train.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5d3f852d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras input shape: (None, 280, 40)\n",
            "Keras output shape: (None, 2)\n",
            "Keras dtype: float32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Aman\\AppData\\Local\\Temp\\tmpqeqs3onj\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Aman\\AppData\\Local\\Temp\\tmpqeqs3onj\\assets\n",
            "c:\\Users\\Aman\\anaconda3\\envs\\tf_gpu_final\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote cough_cnn1_int8.tflite (size KB): 57.734375\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "model = tf.keras.models.load_model(\"cough_cnn1.h5\")\n",
        "print(\"Keras input shape:\", model.input_shape) \n",
        "print(\"Keras output shape:\", model.output_shape)\n",
        "print(\"Keras dtype:\", model.dtype)\n",
        "\n",
        "# --- Representative generator using model.input_shape ---\n",
        "input_shape = model.input_shape  # tuple (None, dim1, dim2?) etc\n",
        "# create a safe generator that uses X_train (you already have X_train)\n",
        "def representative_data_gen():\n",
        "    for i in range(100):\n",
        "        sample = X_train[i].astype(np.float32)\n",
        "        # Ensure sample includes batch dimension\n",
        "        if sample.ndim == len(input_shape) - 1:\n",
        "            sample = sample.reshape((1,) + sample.shape)\n",
        "        elif sample.ndim == len(input_shape):\n",
        "            sample = sample.reshape((1,) + sample.shape[1:])\n",
        "        yield [sample]\n",
        "\n",
        "# Convert to TFLite INT8 (FULL integer quantization, INT8 I/O)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "open(\"cough_cnn1_int8.tflite\",\"wb\").write(tflite_model)\n",
        "print(\"Wrote cough_cnn1_int8.tflite (size KB):\", os.path.getsize(\"cough_cnn1_int8.tflite\")/1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e9d104dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: [  1 280  40]\n",
            "Input quantization: (0.13141827285289764, -1)\n",
            "Output shape: [1 2]\n",
            "Output quantization: (0.00390625, -128)\n",
            "Output dtype: <class 'numpy.int8'>\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"cough_cnn1_int8.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "print(\"Input shape:\", input_details[\"shape\"])\n",
        "print(\"Input quantization:\", input_details[\"quantization\"])   # (scale, zero_point)\n",
        "print(\"Output shape:\", output_details[\"shape\"])\n",
        "print(\"Output quantization:\", output_details[\"quantization\"]) # (scale, zero_point)\n",
        "print(\"Output dtype:\", output_details[\"dtype\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c5099938",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1300  246]\n",
            " [ 266 2743]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "01536414",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
        "disp.plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da7913f2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf_gpu_final",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
