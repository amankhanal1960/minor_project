{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832cc582",
   "metadata": {},
   "source": [
    "# Cough Detection (5s) - Clean Training Pipeline\n",
    "\n",
    "This notebook follows a simple flow similar to the old 9s notebook:\n",
    "1. Import and label dataset from metadata.\n",
    "2. Truncate each clip to 5 seconds.\n",
    "3. Extract MFCC features.\n",
    "4. Train the model.\n",
    "5. Evaluate and export (`.h5` and int8 `.tflite`).\n",
    "\n",
    "Noise robustness is added in training through waveform augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6851b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7484853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public dataset: E:\\minor-project\\public_dataset\n",
      "ESP32 noise dir: E:\\minor-project\\model\\esp32_dataset\\non_cough\n",
      "Target clip: 5.0 seconds\n",
      "Expected MFCC shape: (155, 40)\n",
      "USE_BACKGROUND_NOISE_MIX: False\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Configuration\n",
    "# =====================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "PUBLIC_DATASET_CANDIDATES = [Path('../public_dataset'), Path('public_dataset')]\n",
    "ESP32_NOISE_CANDIDATES = [Path('./esp32_dataset/non_cough'), Path('model/esp32_dataset/non_cough')]\n",
    "\n",
    "\n",
    "def first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return paths[0]\n",
    "\n",
    "\n",
    "PUBLIC_DATASET_DIR = first_existing(PUBLIC_DATASET_CANDIDATES)\n",
    "ESP32_NOISE_DIR = first_existing(ESP32_NOISE_CANDIDATES)\n",
    "\n",
    "OUTPUT_PREFIX = 'cough_cnn_5s_base'\n",
    "\n",
    "# Strong labels from public metadata\n",
    "POS_THRESHOLD = 0.80  # cough_detected >= 0.80 -> cough\n",
    "NEG_THRESHOLD = 0.20  # cough_detected <= 0.20 -> non-cough\n",
    "\n",
    "# Audio + MFCC\n",
    "SR = 16000\n",
    "DURATION = 5.0\n",
    "TARGET_SAMPLES = int(SR * DURATION)\n",
    "\n",
    "N_MFCC = 40\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 512\n",
    "EXPECTED_FRAMES = 1 + int(np.floor((TARGET_SAMPLES - N_FFT) / float(HOP_LENGTH)))\n",
    "\n",
    "# Split\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE_FROM_TRAIN = 0.1765  # gives ~15% total val when test is 15%\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Optional caps for quick experiments (set None for full dataset)\n",
    "MAX_TRAIN_PER_CLASS = None\n",
    "MAX_VAL_PER_CLASS = None\n",
    "MAX_TEST_PER_CLASS = None\n",
    "\n",
    "# Data augmentation (training only)\n",
    "# IMPORTANT: keep this conservative to avoid learning \"noise => cough\" shortcuts.\n",
    "AUG_PER_SAMPLE = 1\n",
    "USE_BACKGROUND_NOISE_MIX = False  # set True only if you want explicit background-noise mixing\n",
    "\n",
    "print('Public dataset:', PUBLIC_DATASET_DIR.resolve())\n",
    "print('ESP32 noise dir:', ESP32_NOISE_DIR.resolve() if ESP32_NOISE_DIR.exists() else ESP32_NOISE_DIR)\n",
    "print('Target clip:', DURATION, 'seconds')\n",
    "print('Expected MFCC shape:', (EXPECTED_FRAMES, N_MFCC))\n",
    "print('USE_BACKGROUND_NOISE_MIX:', USE_BACKGROUND_NOISE_MIX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd1586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usable samples: 17839\n",
      "Class distribution: {1: 12290, 0: 5549}\n",
      "                                            wav_path  cough_score  label\n",
      "0  E:\\minor-project\\public_dataset\\2d22665d-5ca0-...       0.8796      1\n",
      "1  E:\\minor-project\\public_dataset\\2d25616a-2bee-...       0.0099      0\n",
      "2  E:\\minor-project\\public_dataset\\2d26833b-337e-...       0.9817      1\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Step 1: Build metadata table\n",
    "# =====================\n",
    "\n",
    "def build_dataframe(dataset_dir, pos_threshold=0.8, neg_threshold=0.2):\n",
    "    files = []\n",
    "    cough_scores = []\n",
    "    labels = []\n",
    "\n",
    "    for wav_path in sorted(dataset_dir.glob('*.wav')):\n",
    "        json_path = wav_path.with_suffix('.json')\n",
    "        if not json_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            meta = json.loads(json_path.read_text(encoding='utf-8'))\n",
    "            score = float(meta.get('cough_detected'))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if score >= pos_threshold:\n",
    "            label = 1\n",
    "        elif score <= neg_threshold:\n",
    "            label = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        files.append(str(wav_path.resolve()))\n",
    "        cough_scores.append(score)\n",
    "        labels.append(label)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'wav_path': files,\n",
    "        'cough_score': cough_scores,\n",
    "        'label': labels,\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "df = build_dataframe(PUBLIC_DATASET_DIR, POS_THRESHOLD, NEG_THRESHOLD)\n",
    "\n",
    "print('Total usable samples:', len(df))\n",
    "print('Class distribution:', df['label'].value_counts().to_dict())\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399cd8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (12486, 3) {1: 8602, 0: 3884}\n",
      "Val:   (2677, 3) {1: 1844, 0: 833}\n",
      "Test:  (2676, 3) {1: 1844, 0: 832}\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Step 2: Split train/val/test\n",
    "# =====================\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=VAL_SIZE_FROM_TRAIN,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "\n",
    "def cap_per_class(split_df, max_per_class, seed=42):\n",
    "    if max_per_class is None:\n",
    "        return split_df.reset_index(drop=True)\n",
    "\n",
    "    return (\n",
    "        split_df.groupby('label', group_keys=False)\n",
    "        .apply(lambda g: g.sample(n=min(len(g), int(max_per_class)), random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = cap_per_class(train_df, MAX_TRAIN_PER_CLASS, SEED)\n",
    "val_df = cap_per_class(val_df, MAX_VAL_PER_CLASS, SEED)\n",
    "test_df = cap_per_class(test_df, MAX_TEST_PER_CLASS, SEED)\n",
    "\n",
    "print('Train:', train_df.shape, train_df['label'].value_counts().to_dict())\n",
    "print('Val:  ', val_df.shape, val_df['label'].value_counts().to_dict())\n",
    "print('Test: ', test_df.shape, test_df['label'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb8f78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public non-cough noise pool: 3884\n",
      "ESP32 non-cough noise pool: 288\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Step 3: Noise pool + augmentation\n",
    "# =====================\n",
    "\n",
    "PUBLIC_NOISE_PATHS = train_df.loc[train_df['label'] == 0, 'wav_path'].tolist()\n",
    "ESP32_NOISE_PATHS = sorted(str(p.resolve()) for p in ESP32_NOISE_DIR.glob('*.wav')) if ESP32_NOISE_DIR.exists() else []\n",
    "\n",
    "print('Public non-cough noise pool:', len(PUBLIC_NOISE_PATHS))\n",
    "print('ESP32 non-cough noise pool:', len(ESP32_NOISE_PATHS))\n",
    "\n",
    "\n",
    "def pad_or_trim(y, target_samples):\n",
    "    if len(y) < target_samples:\n",
    "        y = np.pad(y, (0, target_samples - len(y)))\n",
    "    elif len(y) > target_samples:\n",
    "        y = y[:target_samples]\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "def rms(x):\n",
    "    return float(np.sqrt(np.mean(np.square(x), dtype=np.float64) + 1e-10))\n",
    "\n",
    "\n",
    "def mix_at_snr(clean, noise, snr_db):\n",
    "    clean_rms = max(rms(clean), 1e-4)\n",
    "    noise_rms = max(rms(noise), 1e-6)\n",
    "    noise_target_rms = clean_rms / (10.0 ** (snr_db / 20.0))\n",
    "    return clean + noise * (noise_target_rms / noise_rms)\n",
    "\n",
    "\n",
    "def sample_noise_clip(target_samples, rng):\n",
    "    all_noise = []\n",
    "    if len(PUBLIC_NOISE_PATHS) > 0:\n",
    "        all_noise.extend(PUBLIC_NOISE_PATHS)\n",
    "    if len(ESP32_NOISE_PATHS) > 0:\n",
    "        all_noise.extend(ESP32_NOISE_PATHS)\n",
    "\n",
    "    if len(all_noise) == 0:\n",
    "        return np.zeros(target_samples, dtype=np.float32)\n",
    "\n",
    "    selected = all_noise[rng.integers(0, len(all_noise))]\n",
    "    y, _ = librosa.load(selected, sr=SR, mono=True)\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    if len(y) >= target_samples:\n",
    "        start = int(rng.integers(0, len(y) - target_samples + 1))\n",
    "        return y[start:start + target_samples]\n",
    "\n",
    "    reps = int(np.ceil(target_samples / len(y)))\n",
    "    return np.tile(y, reps)[:target_samples]\n",
    "\n",
    "\n",
    "def augment_waveform(y, rng):\n",
    "    y = y.astype(np.float32).copy()\n",
    "\n",
    "    # Conservative augmentation to avoid learning loudness shortcuts.\n",
    "    y *= rng.uniform(0.80, 1.20)\n",
    "\n",
    "    if rng.random() < 0.70:\n",
    "        max_shift = int(0.15 * SR)\n",
    "        shift = int(rng.integers(-max_shift, max_shift + 1))\n",
    "        y = np.roll(y, shift)\n",
    "\n",
    "    # Optional background noise mix (disabled by default).\n",
    "    if USE_BACKGROUND_NOISE_MIX and rng.random() < 0.50:\n",
    "        n = sample_noise_clip(len(y), rng)\n",
    "        snr_db = rng.uniform(8.0, 25.0)\n",
    "        y = mix_at_snr(y, n, snr_db)\n",
    "\n",
    "    # Tiny sensor-like hiss only (very mild).\n",
    "    if rng.random() < 0.20:\n",
    "        y += rng.uniform(0.0002, 0.0020) * rng.normal(0.0, 1.0, len(y)).astype(np.float32)\n",
    "\n",
    "    return np.clip(y, -1.0, 1.0).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce35d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Step 4: MFCC extraction (5s)\n",
    "# =====================\n",
    "\n",
    "def extract_mfcc_2d(wav_path=None,\n",
    "                    y=None,\n",
    "                    sr=SR,\n",
    "                    n_mfcc=N_MFCC,\n",
    "                    n_mels=N_MELS,\n",
    "                    n_fft=N_FFT,\n",
    "                    hop_length=HOP_LENGTH,\n",
    "                    duration=DURATION,\n",
    "                    max_frames=EXPECTED_FRAMES,\n",
    "                    normalise=True):\n",
    "    \"\"\"\n",
    "    Extract MFCC as (frames, n_mfcc) from a 5s clip.\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        y, _ = librosa.load(wav_path, sr=sr, mono=True, offset=0.0, duration=duration)\n",
    "\n",
    "    y = pad_or_trim(y, int(sr * duration))\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        htk=False\n",
    "    )\n",
    "\n",
    "    mfcc = mfcc.T.astype(np.float32)\n",
    "\n",
    "    if max_frames is not None:\n",
    "        if mfcc.shape[0] < max_frames:\n",
    "            pad = np.zeros((max_frames - mfcc.shape[0], n_mfcc), dtype=np.float32)\n",
    "            mfcc = np.vstack([mfcc, pad])\n",
    "        elif mfcc.shape[0] > max_frames:\n",
    "            mfcc = mfcc[:max_frames, :]\n",
    "\n",
    "    if normalise:\n",
    "        mean = np.mean(mfcc, axis=0, keepdims=True)\n",
    "        std = np.std(mfcc, axis=0, keepdims=True) + 1e-6\n",
    "        mfcc = (mfcc - mean) / std\n",
    "\n",
    "    return mfcc.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18401bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df,\n",
    "                     duration=DURATION,\n",
    "                     normalise=True,\n",
    "                     augment=False,\n",
    "                     aug_per_sample=0,\n",
    "                     seed=42):\n",
    "    \"\"\"\n",
    "    Build X, y arrays from dataframe.\n",
    "    If augment=True, generates additional noisy samples per training clip.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    n_files = len(df)\n",
    "    total = n_files * (1 + aug_per_sample if augment else 1)\n",
    "\n",
    "    X = np.zeros((total, EXPECTED_FRAMES, N_MFCC), dtype=np.float32)\n",
    "    y = np.zeros((total,), dtype=np.int32)\n",
    "\n",
    "    idx = 0\n",
    "    for row in tqdm(df.itertuples(index=False), total=n_files):\n",
    "        wav_path = row.wav_path\n",
    "        label = int(row.label)\n",
    "\n",
    "        base_wave_raw, _ = librosa.load(wav_path, sr=SR, mono=True, offset=0.0, duration=duration)\n",
    "        base_wave_raw = pad_or_trim(base_wave_raw, TARGET_SAMPLES)\n",
    "\n",
    "        base_wave = base_wave_raw.astype(np.float32)\n",
    "\n",
    "        # original\n",
    "        X[idx] = extract_mfcc_2d(y=base_wave, duration=duration, normalise=normalise)\n",
    "        y[idx] = label\n",
    "        idx += 1\n",
    "\n",
    "        # noisy copies (training only)\n",
    "        if augment:\n",
    "            for _ in range(aug_per_sample):\n",
    "                aug_wave = augment_waveform(base_wave_raw, rng)\n",
    "                aug_wave = aug_wave.astype(np.float32)\n",
    "                X[idx] = extract_mfcc_2d(y=aug_wave, duration=duration, normalise=normalise)\n",
    "                y[idx] = label\n",
    "                idx += 1\n",
    "\n",
    "    return X[:idx], y[:idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4c773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12486/12486 [02:00<00:00, 103.36it/s]\n",
      "100%|██████████| 2677/2677 [00:14<00:00, 186.70it/s]\n",
      "100%|██████████| 2676/2676 [00:12<00:00, 213.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (24972, 155, 40) y_train: (24972,)\n",
      "X_val:   (2677, 155, 40) y_val:   (2677,)\n",
      "X_test:  (2676, 155, 40) y_test:  (2676,)\n",
      "Train class counts: {0: 7768, 1: 17204}\n",
      "Val class counts:   {0: 833, 1: 1844}\n",
      "Test class counts:  {0: 832, 1: 1844}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "X_train, y_train = extract_features(\n",
    "    train_df,\n",
    "    duration=DURATION,\n",
    "    normalise=True,\n",
    "    augment=True,\n",
    "    aug_per_sample=AUG_PER_SAMPLE,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "X_val, y_val = extract_features(\n",
    "    val_df,\n",
    "    duration=DURATION,\n",
    "    normalise=True,\n",
    "    augment=False,\n",
    "    aug_per_sample=0,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "X_test, y_test = extract_features(\n",
    "    test_df,\n",
    "    duration=DURATION,\n",
    "    normalise=True,\n",
    "    augment=False,\n",
    "    aug_per_sample=0,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print('X_train:', X_train.shape, 'y_train:', y_train.shape)\n",
    "print('X_val:  ', X_val.shape, 'y_val:  ', y_val.shape)\n",
    "print('X_test: ', X_test.shape, 'y_test: ', y_test.shape)\n",
    "\n",
    "print('Train class counts:', {0:int(np.sum(y_train==0)), 1:int(np.sum(y_train==1))})\n",
    "print('Val class counts:  ', {0:int(np.sum(y_val==0)), 1:int(np.sum(y_val==1))})\n",
    "print('Test class counts: ', {0:int(np.sum(y_test==0)), 1:int(np.sum(y_test==1))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c178bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.607363542739444, 1: 0.7257614508253895}\n"
     ]
    }
   ],
   "source": [
    "# Prepare labels for categorical training\n",
    "y_train_onehot = to_categorical(y_train, num_classes=2)\n",
    "y_val_onehot = to_categorical(y_val, num_classes=2)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight = {0: float(class_weights[0]), 1: float(class_weights[1])}\n",
    "\n",
    "print('Class weights:', class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c0ca319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 155, 40)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 155, 32)           3872      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 155, 32)          128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 77, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 77, 64)            6208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 77, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 38, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 38, 128)           24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 38, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,066\n",
      "Trainable params: 43,618\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.8198\n",
      "Epoch 1: val_loss improved from inf to 0.35107, saving model to cough_cnn_5s_base.h5\n",
      "781/781 [==============================] - 9s 8ms/step - loss: 0.4326 - accuracy: 0.8198 - val_loss: 0.3511 - val_accuracy: 0.8756 - lr: 3.0000e-04\n",
      "Epoch 2/40\n",
      "779/781 [============================>.] - ETA: 0s - loss: 0.3406 - accuracy: 0.8814\n",
      "Epoch 2: val_loss did not improve from 0.35107\n",
      "781/781 [==============================] - 6s 8ms/step - loss: 0.3408 - accuracy: 0.8813 - val_loss: 0.3547 - val_accuracy: 0.8760 - lr: 3.0000e-04\n",
      "Epoch 3/40\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.8948\n",
      "Epoch 3: val_loss did not improve from 0.35107\n",
      "781/781 [==============================] - 17s 21ms/step - loss: 0.3152 - accuracy: 0.8948 - val_loss: 0.3556 - val_accuracy: 0.8722 - lr: 3.0000e-04\n",
      "Epoch 4/40\n",
      "774/781 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.9052\n",
      "Epoch 4: val_loss improved from 0.35107 to 0.33653, saving model to cough_cnn_5s_base.h5\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 0.2984 - accuracy: 0.9053 - val_loss: 0.3365 - val_accuracy: 0.8827 - lr: 3.0000e-04\n",
      "Epoch 5/40\n",
      "777/781 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9128\n",
      "Epoch 5: val_loss did not improve from 0.33653\n",
      "781/781 [==============================] - 6s 8ms/step - loss: 0.2846 - accuracy: 0.9128 - val_loss: 0.3460 - val_accuracy: 0.8816 - lr: 3.0000e-04\n",
      "Epoch 6/40\n",
      "781/781 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9216\n",
      "Epoch 6: val_loss did not improve from 0.33653\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2720 - accuracy: 0.9216 - val_loss: 0.3457 - val_accuracy: 0.8894 - lr: 3.0000e-04\n",
      "Epoch 7/40\n",
      "777/781 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9271\n",
      "Epoch 7: val_loss did not improve from 0.33653\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2611 - accuracy: 0.9270 - val_loss: 0.3858 - val_accuracy: 0.8704 - lr: 3.0000e-04\n",
      "Epoch 8/40\n",
      "777/781 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9357\n",
      "Epoch 8: val_loss did not improve from 0.33653\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2480 - accuracy: 0.9356 - val_loss: 0.3654 - val_accuracy: 0.8827 - lr: 3.0000e-04\n",
      "Epoch 9/40\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9447\n",
      "Epoch 9: val_loss did not improve from 0.33653\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2325 - accuracy: 0.9447 - val_loss: 0.3622 - val_accuracy: 0.8808 - lr: 1.5000e-04\n",
      "Epoch 10/40\n",
      "775/781 [============================>.] - ETA: 0s - loss: 0.2243 - accuracy: 0.9503\n",
      "Epoch 10: val_loss did not improve from 0.33653\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2243 - accuracy: 0.9502 - val_loss: 0.3684 - val_accuracy: 0.8835 - lr: 1.5000e-04\n",
      "Epoch 11/40\n",
      "774/781 [============================>.] - ETA: 0s - loss: 0.2207 - accuracy: 0.9511\n",
      "Epoch 11: val_loss did not improve from 0.33653\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2212 - accuracy: 0.9509 - val_loss: 0.3640 - val_accuracy: 0.8846 - lr: 1.5000e-04\n",
      "Epoch 12/40\n",
      "777/781 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9541\n",
      "Epoch 12: val_loss did not improve from 0.33653\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "781/781 [==============================] - 5s 7ms/step - loss: 0.2165 - accuracy: 0.9539 - val_loss: 0.3968 - val_accuracy: 0.8737 - lr: 1.5000e-04\n",
      "Epoch 13/40\n",
      "779/781 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.9597\n",
      "Epoch 13: val_loss did not improve from 0.33653\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2077 - accuracy: 0.9597 - val_loss: 0.3688 - val_accuracy: 0.8849 - lr: 7.5000e-05\n",
      "Epoch 14/40\n",
      "776/781 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9611\n",
      "Epoch 14: val_loss did not improve from 0.33653\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "781/781 [==============================] - 6s 7ms/step - loss: 0.2059 - accuracy: 0.9610 - val_loss: 0.3736 - val_accuracy: 0.8823 - lr: 7.5000e-05\n",
      "Epoch 14: early stopping\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Step 5: Model + training\n",
    "# =====================\n",
    "\n",
    "def build_tinyml_cnn(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = layers.Dropout(0.30)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.20)(x)\n",
    "\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "model = build_tinyml_cnn(input_shape)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'{OUTPUT_PREFIX}.h5',\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=4,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_onehot,\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(f'{OUTPUT_PREFIX}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c02e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 4ms/step\n",
      "Best threshold from val: 0.315\n",
      "Val metrics @best threshold: {'precision': 0.9172450613988254, 'recall': 0.9316702819956616, 'f1': 0.9244013989776702}\n",
      "84/84 [==============================] - 0s 4ms/step\n",
      "Confusion matrix (test):\n",
      "[[ 679  153]\n",
      " [ 116 1728]]\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8541    0.8161    0.8347       832\n",
      "           1     0.9187    0.9371    0.9278      1844\n",
      "\n",
      "    accuracy                         0.8995      2676\n",
      "   macro avg     0.8864    0.8766    0.8812      2676\n",
      "weighted avg     0.8986    0.8995    0.8988      2676\n",
      "\n",
      "Test AUC: 0.9526257195895211\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Step 6: Validation threshold + Test evaluation\n",
    "# =====================\n",
    "val_prob = model.predict(X_val, batch_size=BATCH_SIZE, verbose=1)[:, 1]\n",
    "\n",
    "best_thr = None\n",
    "best_row = None\n",
    "rows = []\n",
    "\n",
    "for thr in np.linspace(0.10, 0.90, 161):\n",
    "    pred = (val_prob >= thr).astype(np.int32)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_val, pred, average='binary', zero_division=0)\n",
    "    rows.append((float(thr), float(p), float(r), float(f1)))\n",
    "\n",
    "thr_df = pd.DataFrame(rows, columns=['threshold', 'precision', 'recall', 'f1'])\n",
    "\n",
    "# prefer thresholds with precision >= 0.80 to reduce false cough detections\n",
    "candidates = thr_df[thr_df['precision'] >= 0.80]\n",
    "if len(candidates) > 0:\n",
    "    best_row = candidates.sort_values(['f1', 'precision', 'recall'], ascending=False).iloc[0]\n",
    "else:\n",
    "    best_row = thr_df.sort_values(['f1', 'precision', 'recall'], ascending=False).iloc[0]\n",
    "\n",
    "best_thr = float(best_row['threshold'])\n",
    "\n",
    "print('Best threshold from val:', best_thr)\n",
    "print('Val metrics @best threshold:', {\n",
    "    'precision': float(best_row['precision']),\n",
    "    'recall': float(best_row['recall']),\n",
    "    'f1': float(best_row['f1'])\n",
    "})\n",
    "\n",
    "test_prob = model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)[:, 1]\n",
    "test_pred = (test_prob >= best_thr).astype(np.int32)\n",
    "\n",
    "print('Confusion matrix (test):')\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "\n",
    "print('Classification report (test):')\n",
    "print(classification_report(y_test, test_pred, digits=4))\n",
    "\n",
    "print('Test AUC:', roc_auc_score(y_test, test_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001ae37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Aman\\AppData\\Local\\Temp\\tmpdnf5p1e6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Aman\\AppData\\Local\\Temp\\tmpdnf5p1e6\\assets\n",
      "c:\\Users\\Aman\\anaconda3\\envs\\tf_gpu_final\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite model: E:\\minor-project\\model\\cough_cnn_5s_base_int8.tflite\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Step 7: Convert to int8 TFLite\n",
    "# =====================\n",
    "\n",
    "def representative_data_gen():\n",
    "    n = min(300, len(X_train))\n",
    "    idx = np.random.choice(len(X_train), size=n, replace=False)\n",
    "    for i in idx:\n",
    "        sample = X_train[i:i+1].astype(np.float32)\n",
    "        yield [sample]\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "quant_tflite = converter.convert()\n",
    "\n",
    "model_tflite_path = Path(f'{OUTPUT_PREFIX}_int8.tflite')\n",
    "model_tflite_path.write_bytes(quant_tflite)\n",
    "print('Saved TFLite model:', model_tflite_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69dc6d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [  1 155  40]\n",
      "Input quantization (scale, zero_point): (0.09420423954725266, -1)\n",
      "Input dtype: <class 'numpy.int8'>\n",
      "Output shape: [1 2]\n",
      "Output quantization (scale, zero_point): (0.00390625, -128)\n",
      "Output dtype: <class 'numpy.int8'>\n"
     ]
    }
   ],
   "source": [
    "# Check quantization parameters (needed for Arduino)\n",
    "interpreter = tf.lite.Interpreter(model_path=f'{OUTPUT_PREFIX}_int8.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print('Input shape:', input_details['shape'])\n",
    "print('Input quantization (scale, zero_point):', input_details['quantization'])\n",
    "print('Input dtype:', input_details['dtype'])\n",
    "\n",
    "print('Output shape:', output_details['shape'])\n",
    "print('Output quantization (scale, zero_point):', output_details['quantization'])\n",
    "print('Output dtype:', output_details['dtype'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
